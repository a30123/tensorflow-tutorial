{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "# 設定環境變數來控制 keras, theano\n",
    "os.environ['KERAS_BACKEND']=\"tensorflow\"\n",
    "#os.environ['THEANO_FLAGS']=\"floatX=float32, device=cuda\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.models import Sequential\n",
    "from PIL import Image\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import keras.backend as K\n",
    "# 設定 channels_first 或 channels_last\n",
    "K.set_image_data_format('channels_last')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import load_img\n",
    "from IPython.display import display\n",
    "img_H, img_W = 360, 480\n",
    "def preprocess_image(filename):\n",
    "    img = np.array(load_img(filename, target_size=(img_H, img_W)))    \n",
    "    img = img[None, ...].astype('float32')\n",
    "    img = keras.applications.vgg16.preprocess_input(img)\n",
    "    return img\n",
    "def show_image(arr):\n",
    "    arr = arr.reshape(img_H, img_W,3)+[103.939, 116.779, 123.68]\n",
    "    arr = arr.clip(0,255).astype('uint8')[:,:, ::-1]\n",
    "    display(Image.fromarray(arr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "from keras.engine.topology import Layer\n",
    "import numpy as np\n",
    "\n",
    "class ImageLayer(Layer):\n",
    "\n",
    "    def __init__(self,  init_img=None, **kwargs):\n",
    "        if init_img is None:\n",
    "            self.init_img = np.random.uniform(-50,50,size=(1,img_H, img_W, 3)).astype('float32')\n",
    "        else:\n",
    "            self.init_img = init_img\n",
    "        super().__init__(**kwargs)\n",
    "    def initializer(self, size):\n",
    "        return self.init_img\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        # Create a trainable weight variable for this layer.         \n",
    "        self.img = self.add_weight(shape=(1, img_H, img_W, 3),                                   \n",
    "                                    initializer=self.initializer,\n",
    "                                    trainable=True)\n",
    "        super().build(input_shape)  # Be sure to call this somewhere!\n",
    "        \n",
    "    def call(self, x):\n",
    "        return self.img\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (1, img_H, img_W, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 結構的圖片\n",
    "#base_image = preprocess_image(\"img/tubingen.jpg\")\n",
    "base_image = preprocess_image(\"img/tubingen.jpg\")\n",
    "show_image(base_image)\n",
    "style_image = preprocess_image(\"img/starry_night.jpg\")\n",
    "show_image(style_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "image_layer = ImageLayer( init_img=.9*base_image +.1*style_image,\n",
    "                         name='image_layer')(keras.layers.Input(shape=(0,)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Hack\n",
    "_load_weights = keras.models.Model.load_weights\n",
    "def my_load_weights(self, fn):\n",
    "    return _load_weights(self, fn, by_name=True)\n",
    "keras.models.Model.load_weights = my_load_weights\n",
    "\n",
    "# 將以上三個圖片送入 vgg16\n",
    "vgg16_model = keras.applications.vgg16.VGG16(weights='imagenet', input_tensor=image_layer,\n",
    "                                             include_top=False, input_shape=(img_H, img_W,3))\n",
    "# unhack\n",
    "keras.models.Model.load_weights = _load_weights\n",
    "\n",
    "# 比較簡單的方式取得各層\n",
    "outputs_dict = {layer.name :layer.output for layer in vgg16_model.layers }\n",
    "outputs_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "w = vgg16_model.get_layer('image_layer').weights[0]\n",
    "style_feature_names = ['block1_conv1', 'block2_conv1',\n",
    "                  'block3_conv1', 'block4_conv1',\n",
    "                  'block5_conv1']\n",
    "style_features = [outputs_dict[x] for x in style_feature_names]\n",
    "content_feature = outputs_dict['block4_conv2']\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())    \n",
    "    target_content_feature = sess.run(content_feature, feed_dict={w: base_image}) \n",
    "    target_style_features = sess.run(style_features, feed_dict={w: style_image}) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 各種 Norms 和 loss function\n",
    "# 取自 https://github.com/fchollet/keras/blob/master/examples/neural_style_transfer.py\n",
    "# compute the neural style loss\n",
    "# first we need to define 4 util functions\n",
    "\n",
    "# the gram matrix of an image tensor (feature-wise outer product)\n",
    "def gram_matrix(x):\n",
    "    assert K.ndim(x) == 3\n",
    "    features = K.batch_flatten(K.permute_dimensions(x, (2, 0, 1)))\n",
    "    gram = K.dot(features, K.transpose(features))\n",
    "    return gram\n",
    "\n",
    "# the \"style loss\" is designed to maintain\n",
    "# the style of the reference image in the generated image.\n",
    "# It is based on the gram matrices (which capture style) of\n",
    "# feature maps from the style reference image\n",
    "# and from the generated image\n",
    "\n",
    "def style_loss(combination, target):\n",
    "    assert K.ndim(combination) == 3\n",
    "    assert np.ndim(target) ==3\n",
    "    S = gram_matrix(K.constant(target))\n",
    "    C = gram_matrix(combination)    \n",
    "    size = target.size\n",
    "    return K.sum(K.square(S - C)) / (4. * (size ** 2))\n",
    "\n",
    "# an auxiliary loss function\n",
    "# designed to maintain the \"content\" of the\n",
    "# base image in the generated image\n",
    "\n",
    "def content_loss(combination, target):\n",
    "    assert np.ndim(target) ==3\n",
    "    assert K.ndim(combination) == 3\n",
    "    size = target.size\n",
    "    return K.sum(K.square(combination - K.constant(target)))/size\n",
    "\n",
    "# the 3rd loss function, total variation loss,\n",
    "# designed to keep the generated image locally coherent\n",
    "\n",
    "def total_variation_loss(x):\n",
    "    assert K.ndim(x) == 4\n",
    "    a = K.square(x[:, :-1, :-1, :] - x[:, 1: , :-1, :])\n",
    "    b = K.square(x[:, :-1, :-1, :] - x[:, :-1, 1: , :])\n",
    "    size = img_H * img_W * 3\n",
    "    return K.sum(K.pow(a + b, 1.25))/size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "content_weight = .5\n",
    "style_weight = 1.0\n",
    "total_variation_weight = 1e-6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#content_weight = 20\n",
    "#style_weight = 1.0\n",
    "#total_variation_weight = 5e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "loss_c = content_loss(content_feature[0], target_content_feature[0])\n",
    "loss_s = K.variable(0.)\n",
    "for layer, target_layer in zip(style_features, target_style_features):\n",
    "    loss_s = 2*loss_s + style_loss(layer[0], target_layer[0])\n",
    "loss_s /= len(style_features)\n",
    "loss_t = total_variation_loss(outputs_dict['image_layer'])\n",
    "loss = content_weight * loss_c + style_weight*loss_s + total_variation_weight * loss_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#train_step = tf.train.AdamOptimizer(5e-2).minimize(loss, var_list=[w])\n",
    "train_step = tf.train.AdamOptimizer(0.1).minimize(loss, var_list=[w])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    tf.global_variables_initializer().run()\n",
    "    for i in range(50000):\n",
    "        if i%100==0:\n",
    "            if i%500==0:\n",
    "                show_image(w.eval())\n",
    "            print(i, sess.run([loss, loss_s, loss_c, loss_t]))\n",
    "        train_step.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 參考結果\n",
    "<img src=\"img/result.png\" />"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
