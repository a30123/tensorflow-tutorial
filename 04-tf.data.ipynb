{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using tf.data\n",
    "\n",
    "https://www.tensorflow.org/programmers_guide/datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tjw/anaconda3/envs/keras/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<dtype: 'int64'>\n",
      "()\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n"
     ]
    }
   ],
   "source": [
    "dataset = tf.data.Dataset.range(10)\n",
    "print(dataset.output_types)\n",
    "print(dataset.output_shapes)\n",
    "\n",
    "iterator = dataset.make_one_shot_iterator()\n",
    "next_element = iterator.get_next()\n",
    "with tf.Session() as sess:\n",
    "    for i in range(10):\n",
    "        print(sess.run(next_element))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### make_initializable_iterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "min_value = tf.placeholder(tf.int64, shape=[])\n",
    "max_value = tf.placeholder(tf.int64, shape=[])\n",
    "dataset = tf.data.Dataset.range(min_value, max_value)\n",
    "iterator = dataset.make_initializable_iterator()\n",
    "next_element = iterator.get_next()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    # Initialize an iterator over a dataset with 10 elements.\n",
    "    sess.run(iterator.initializer, feed_dict={min_value: 0, max_value: 5})\n",
    "    for i in range(5):\n",
    "        print(sess.run(next_element))\n",
    "\n",
    "    # Initialize the same iterator over a dataset with 10 elements.\n",
    "    sess.run(iterator.initializer, feed_dict={min_value: 100, max_value: 105})\n",
    "    for i in range(5):\n",
    "        value = sess.run(next_element)\n",
    "        print(value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## from_tensor_slices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<dtype: 'float32'>\n",
      "(3,)\n",
      "[0.9010345  0.5810802  0.21208417]\n",
      "[0.79705596 0.32449102 0.22944129]\n",
      "[0.56565714 0.87705386 0.97775614]\n",
      "[0.26984704 0.46767223 0.85091555]\n",
      "[0.21259892 0.31673062 0.17329764]\n",
      "[0.78511834 0.6713352  0.13638711]\n",
      "[0.06600571 0.01555169 0.35010052]\n",
      "[0.40444255 0.394624   0.92135096]\n",
      "[0.0185889 0.8673384 0.5681735]\n",
      "[0.7880877  0.00849426 0.9550835 ]\n"
     ]
    }
   ],
   "source": [
    "dataset = tf.data.Dataset.from_tensor_slices(tf.random_uniform([10, 3]))\n",
    "\n",
    "print(dataset.output_types)\n",
    "print(dataset.output_shapes)\n",
    "\n",
    "iterator = dataset.make_initializable_iterator()\n",
    "\n",
    "next_element = iterator.get_next()\n",
    "with tf.Session() as sess:\n",
    "    sess.run(iterator.initializer)\n",
    "    for i in range(10):\n",
    "        print(sess.run(next_element))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q\n",
    "計算 $1+2+...+10$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reinitializable (one interator with different datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0\n",
      "train 0 7\n",
      "train 1 -1\n",
      "train 2 -1\n",
      "train 3 -6\n",
      "train 4 9\n",
      "train 5 3\n",
      "train 6 8\n",
      "train 7 -1\n",
      "train 8 14\n",
      "train 9 1\n",
      "validation 0 0\n",
      "validation 1 1\n",
      "validation 2 2\n",
      "validation 3 3\n",
      "validation 4 4\n",
      "epoch 1\n",
      "train 0 -2\n",
      "train 1 6\n",
      "train 2 3\n",
      "train 3 11\n",
      "train 4 7\n",
      "train 5 5\n",
      "train 6 -3\n",
      "train 7 15\n",
      "train 8 10\n",
      "train 9 2\n",
      "validation 0 0\n",
      "validation 1 1\n",
      "validation 2 2\n",
      "validation 3 3\n",
      "validation 4 4\n",
      "epoch 2\n",
      "train 0 8\n",
      "train 1 -9\n",
      "train 2 -8\n",
      "train 3 -7\n",
      "train 4 -1\n",
      "train 5 -4\n",
      "train 6 -2\n",
      "train 7 8\n",
      "train 8 9\n",
      "train 9 5\n",
      "validation 0 0\n",
      "validation 1 1\n",
      "validation 2 2\n",
      "validation 3 3\n",
      "validation 4 4\n"
     ]
    }
   ],
   "source": [
    "# Define training and validation datasets with the same structure.\n",
    "training_dataset = tf.data.Dataset.range(10).map(\n",
    "    lambda x: x + tf.random_uniform([], -10, 10, tf.int64))\n",
    "validation_dataset = tf.data.Dataset.range(5)\n",
    "\n",
    "# two dataset are compatible\n",
    "assert training_dataset.output_types == validation_dataset.output_types\n",
    "assert training_dataset.output_shapes == validation_dataset.output_shapes\n",
    "\n",
    "iterator = tf.data.Iterator.from_structure(training_dataset.output_types,\n",
    "                                           training_dataset.output_shapes)\n",
    "next_element = iterator.get_next()\n",
    "\n",
    "training_init_op = iterator.make_initializer(training_dataset)\n",
    "validation_init_op = iterator.make_initializer(validation_dataset)\n",
    "\n",
    "def loop_through_dataset(ds_name, n):\n",
    "        for _ in range(n):\n",
    "            print(ds_name, _, sess.run(next_element))\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    for epoch in range(3):    \n",
    "        print(\"epoch\", epoch)\n",
    "        # training\n",
    "        sess.run(training_init_op)\n",
    "        loop_through_dataset(\"train\", 10)\n",
    "\n",
    "        # Validation\n",
    "        sess.run(validation_init_op)\n",
    "        loop_through_dataset(\"validation\", 5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MNIST Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lzma\n",
    "import pickle\n",
    "with lzma.open(\"mnist.pkl.xz\", 'rb') as f:\n",
    "    train_set, validation_set, test_set = pickle.load(f, encoding='latin1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X, train_y = train_set\n",
    "validation_X, validation_y = validation_set\n",
    "test_X, test_y = test_set\n",
    "train_Y = np.eye(10)[train_y]\n",
    "test_Y = np.eye(10)[test_y]\n",
    "validation_Y = np.eye(10)[validation_y]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=560x28 at 0x18196FEFD0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5 0 4 ... 8 4 8]\n"
     ]
    }
   ],
   "source": [
    "from IPython.display import display\n",
    "def showX(X):\n",
    "    int_X = (X*255).clip(0,255).astype('uint8')\n",
    "    # N*784 -> N*28*28 -> 28*N*28 -> 28 * 28N\n",
    "    int_X_reshape = int_X.reshape(-1,28,28).swapaxes(0,1).reshape(28,-1)\n",
    "    display(Image.fromarray(int_X_reshape))\n",
    "# 訓練資料， X 的前 20 筆\n",
    "showX(train_X[:20])\n",
    "print(train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "[[0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]]\n",
      "[[0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "train_data = tf.data.Dataset.from_tensor_slices((train_X, train_Y))\n",
    "\n",
    "iterator = train_data.batch(4).make_initializable_iterator()\n",
    "\n",
    "next_minibatch = iterator.get_next()\n",
    "with tf.Session() as sess:\n",
    "    sess.run(iterator.initializer)\n",
    "    for i in range(3):\n",
    "        print(sess.run(next_minibatch)[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multilayer Convolutional Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = tf.data.Dataset.from_tensor_slices((train_X, train_Y)).shuffle(buffer_size=10000).batch(40)\n",
    "validation_data = tf.data.Dataset.from_tensor_slices((validation_X, validation_Y)).batch(40)\n",
    "\n",
    "\n",
    "iterator = tf.data.Iterator.from_structure(training_data.output_types,\n",
    "                                           training_data.output_shapes)\n",
    "\n",
    "training_init_op = iterator.make_initializer(training_data)\n",
    "validation_init_op = iterator.make_initializer(validation_data)\n",
    "\n",
    "X, Y_ = iterator.get_next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 設定 weight 和 bais\n",
    "def weight_variable(shape):\n",
    "    initial = tf.truncated_normal(shape, stddev=0.1)\n",
    "    return tf.Variable(initial, name ='W')\n",
    "def bias_variable(shape):\n",
    "    initial = tf.constant(0.1, shape=shape)\n",
    "    return tf.Variable(initial, name = 'b')\n",
    "\n",
    "# 設定 cnn 的 layers\n",
    "def conv2d(X, W):\n",
    "    return tf.nn.conv2d(X, W, strides=[1,1,1,1], padding='SAME')\n",
    "def max_pool_2x2(X):\n",
    "    return tf.nn.max_pool(X, ksize=[1,2,2,1], strides=[1,2,2,1], padding='SAME')\n",
    "\n",
    "# fisrt layer\n",
    "with tf.name_scope('conv1'):\n",
    "    ## variables\n",
    "    W_conv1 = weight_variable([3,3,1,32])\n",
    "    b_conv1 = bias_variable([32])\n",
    "    ## build the layer\n",
    "    X_image = tf.reshape(X, [-1, 28, 28, 1])\n",
    "    h_conv1 = tf.nn.relu(conv2d(X_image, W_conv1) + b_conv1)\n",
    "    h_pool1 = max_pool_2x2(h_conv1)\n",
    "\n",
    "# second layer\n",
    "with tf.name_scope('conv2'):\n",
    "    ## variables\n",
    "    W_conv2 = weight_variable([3,3,32,64])\n",
    "    b_conv2 = bias_variable([64])\n",
    "    ## build the layer\n",
    "    h_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2) + b_conv2)\n",
    "    h_pool2 = max_pool_2x2(h_conv2)\n",
    "    \n",
    "# fully-connected layer\n",
    "with tf.name_scope('full'):\n",
    "    W_fc1 = weight_variable([7*7*64, 1024])\n",
    "    b_fc1 = bias_variable([1024])\n",
    "    h_pool2_flat = tf.reshape(h_pool2, [-1, 7*7*64])\n",
    "    h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1)+b_fc1)\n",
    "    \n",
    "# Dropout:  A Simple Way to Prevent Neural Networks from Over fitting\n",
    "# https://www.cs.toronto.edu/~hinton/absps/JMLRdropout.pdf\n",
    "with tf.name_scope('dropout'):\n",
    "    keep_prob = tf.placeholder(\"float\", name=\"keep_prob\")\n",
    "    h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)\n",
    "\n",
    "# Readout\n",
    "with tf.name_scope('readout'):\n",
    "    W_fc2 = weight_variable([1024,10])\n",
    "    b_fc2 = bias_variable([10])\n",
    "    Y = tf.matmul(h_fc1_drop, W_fc2)+b_fc2\n",
    "    \n",
    "cross_entropy =  tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(labels=Y_, logits=Y))\n",
    "train_step = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy)\n",
    "prediction = tf.argmax(Y, 1, name=\"prediction\")\n",
    "correct_prediction = tf.equal(prediction, tf.argmax(Y_, 1), name=\"correction\")\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32), name=\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of epoch 0 time: 56.79229283332825\n",
      "Epoch 0, validation accuracy 0.9425\n",
      "End of epoch 1 time: 110.72975277900696\n",
      "Epoch 1, validation accuracy 0.965\n",
      "End of epoch 2 time: 168.4089539051056\n",
      "Epoch 2, validation accuracy 0.97\n",
      "End of epoch 3 time: 219.21565794944763\n",
      "Epoch 3, validation accuracy 0.9725\n",
      "End of epoch 4 time: 277.69393396377563\n",
      "Epoch 4, validation accuracy 0.98\n",
      "End of epoch 5 time: 334.8136830329895\n",
      "Epoch 5, validation accuracy 0.9775\n",
      "End of epoch 6 time: 391.15834403038025\n",
      "Epoch 6, validation accuracy 0.985\n",
      "End of epoch 7 time: 445.07757806777954\n",
      "Epoch 7, validation accuracy 0.98\n",
      "End of epoch 8 time: 503.41248393058777\n",
      "Epoch 8, validation accuracy 0.9825\n",
      "End of epoch 9 time: 571.1199989318848\n",
      "Epoch 9, validation accuracy 0.99\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "t0 = time.time()\n",
    "for epoch in range(10):\n",
    "    sess.run(training_init_op)\n",
    "    while True:\n",
    "        try:\n",
    "            sess.run(train_step, {keep_prob: 0.5 })\n",
    "        except tf.errors.OutOfRangeError:\n",
    "            print(\"End of epoch\", epoch, \"time:\", time.time()-t0)\n",
    "            break\n",
    "    sess.run(validation_init_op)\n",
    "    validation_accuracy = np.mean([sess.run(accuracy,{keep_prob: 1.0 }) for i in range(10)])\n",
    "    print(\"Epoch %d, validation accuracy %g\"%(epoch, validation_accuracy))\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
